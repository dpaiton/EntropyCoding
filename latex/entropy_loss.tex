%%%%%%%%
%
% LaTeX document for describing the forward & derivative
% functions for an entropy loss.
%
%  Dylan Paiton
%  Jan 06-2016
%
%%%%%%%%

% Create eps file
\begin{filecontents*}{entropy_loss.eps}
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}

\RequirePackage{fix-cm}

%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn

\smartqed  % flush right qed marks, e.g. at end of proof

\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{mathptmx}      % use Times fonts if available on your TeX system
%\usepackage{latexsym}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\tightoverset}[2]{\mathop{#2}\limits^{\vbox to -.6ex{\kern-0.75ex\hbox{$#1$}\vss}}}

\begin{document}

\title{Entropy loss function for unsupervised deep learning
%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}

%\subtitle{Entropy Loss for Deep Networks}

\titlerunning{Entropy Loss for Deep Networks}        % if too long for running head

\author{
Dylan M. Paiton\\
Sheng Y. Lundquist
}  


\institute{DM Paiton \at
              Vision Science Graduate Group\\
              Redwood Center for Theoretical Neuroscience\\
              University of California, Berkeley\\
              \email{dpaiton@berkeley.edu}           %  \\
}

\maketitle

\begin{abstract}
Here we describe a loss function for training a typical deep network in an unsupervised way. The objective of the loss is to encourage the deep network to minimize the entropy of its output while still conveying information. This should result in an intelligent clustering of object categories without the need of human generated object labels.
\keywords{entropy \and deep network \and loss functions \and unsupervised learning}
\end{abstract}

\section{Introduction} \label{introduction}
\noindent Here we describe a loss function for the unsupervised training of a deep neural network. This is motivated by the cost of labeling data and we want to learn a more general representation.\\

\noindent The goal of the project is to develop a cost function that applies to any generic neural network output. In this document we will first define our loss function in terms of a cost that must be propagated through the deep network to modify parameters. Then we will derive the gradient of the cost function with respect to an individual output element from the deep network. This gradient can then be applied to the network parameters via the back propagation algorithm. We assume that the deep network is being trained via stochastic gradient descent with a batch of images. For each image, we expect an output vector of floats (they could be positive only, but it is not required). We assume one  vector per image in the batch, which can then be assembled into a matrix.

%%%%%%%%%%%%%%%%%%%%%
% Forward Function
%%%%%%%%%%%%%%%%%%%%%
\section{Forward Function - Entropy Computation} \label{forward}
\noindent The input to the entropy loss function is $X$, a matrix of $N$ neurons by $B$ batch inputs. We note that the indices $i, j, k$ will be used to index the node in $X$ and $l,m,n$ will be used to index the batch number in $X$. We want the network output distribution for a given image to be a delta function (i.e. one-hot distribution), indicating that the input falls into a particular category. Additionally, we want the average network output distribution over all images to be uniform. These objectives can be expressed in terms of entropy. First we define a probability distribution across the $N$ nodes in $X$:

\begin{equation} \label{qdef}
    q_{i,l} = \frac{e^{-\beta X_{i,l}}}{\sum_{j}^{N} e^{-\beta X_{j,l}}},
\end{equation}

\noindent where $q$ is computed per neuron for a single input, $i \in N$ indexes the neurons, and $l \in B$ indexes the batch input. For $\beta = -1$, equation \ref{qdef} is the softmax function on $X$. We include the $\beta$ term to allow us to modify the temperature of our probability distribution. Next we define a probability distribution across our batch of $B$ inputs:

\begin{equation} \label{pdef}
    p_{i,l} = \frac{e^{-\beta X_{i,l}}}{\sum_{m}^{B} e^{-\beta X_{i,m}}}.
\end{equation}

\noindent The entropy for a given batch image (summed over nodes) and for a given node (summed over the batch) are

\begin{equation} \label{Hl}
    H_{l} = -\sum_{i} q_{i,l} \ln q_{i,l}
\end{equation}

\noindent and

\begin{equation} \label{Hi}
    H_{i} = -\sum_{l} p_{i,l} \ln p_{i,l},
\end{equation}

\noindent respectively. We wish for all nodes to be active over the corpus of our inputs and for only one node to be active for a single input. Thus, we want to minimize \textbf{positive} entropy per image and minimize \textbf{negative} entropy per batch. This should encourage the network to cluster inputs into discrete categories and produce a sparse output. Therefore, our cost function to minimize will combine equation \ref{Hl} with the negative of equation \ref{Hi}:

\begin{displaymath}
\begin{align}

    C& = \sum_{l}H_{l} - \lambda \sum_{i}H_{i}\\

    C& = - \sum_{l}\sum_{i} q_{i,l} \ln q_{i,l} + \lambda \sum_{i}\sum_{l} p_{i,l} \ln p_{i,l}
\end{align}
\end{displaymath}

\begin{equation} \label{cost}
\begin{aligned}
    C& = \sum_{i,l}\frac{e^{-\beta X_{i,l}}}{\sum_{j}e^{-\beta X_{j,l}}} \left(\beta X_{i,l} + \ln{\sum_{j}e^{-\beta X_{j,l}}}\right) -\\ 
     & \lambda \sum_{i,l}\frac{e^{-\beta X_{i,l}}}{\sum_{m} e^{-\beta X_{i,m}}} \left(\beta X_{i,l} + \ln{\sum_{m}e^{-\beta X_{i,m}}}\right).
\end{aligned}
\end{equation}

\noindent It should be clear here that the two entropy calculations are nearly identical, and therefore the derivatives are going to be similar. We will take the derivative of one entropy function and apply it to both terms in our cost function.

\begin{displaymath}
    H_{l} = - \sum_{i} q_{i,l} \ln q_{i,l}
\end{displaymath}

\begin{displaymath}
    H_{l} =
    \sum_{i}\frac{\beta X_{i,l} e^{-\beta X_{i,l}}}{\sum_{j}e^{-\beta X_{j,l}}} + 
    \sum_{i}\frac{e^{-\beta X_{i,l}}}{\sum_{j}e^{-\beta X_{j,l}}}\ln{\sum_{j}e^{-\beta X_{j,l}}}
\end{displaymath}

\begin{equation} \label{entropyexp}
    H_{l} =
    \overbrace{\sum_{i}\frac{\beta X_{i,l}e^{-\beta X_{i,l}}}{\sum_{j}e^{-\beta X_{j,l}}}}^\text{Left Term} + 
    \overbrace{\frac{\ln{\sum_{j}e^{-\beta X_{j,l}}}}{\sum_{j}e^{-\beta X_{j,l}}}\sum_{i}e^{-\beta X_{i,l}}}^\text{Right Term}.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%
% Backward Function
%%%%%%%%%%%%%%%%%%%%%
\section{Backward Function - Entropy Gradient} \label{backward}
\noindent The gradient of the forward function will contain the partial derivative of equation \ref{cost} with respect to an individual element, $X_{k,l}$. For the sake of brevity, we will only derive a single entorpy term, given in equation \ref{entropyexp}. First we will take the derivative of the left term, as defined in equation \ref{entropyexp}, then we will take the derivative of the right term, then we will combine them.

%%%%%%%%%%%%%%%%%%%%%
% Useful Derivatives 
%%%%%%%%%%%%%%%%%%%%%
\subsection{Useful Derivatives} \label{derivatives}
\noindent The following are a handful of derivatives that have to be taken multiple times for the entropy gradient calculation.

%TODO: Left justify
\begin{equation} \label{dx}
    \frac{\partial\sum_{i}X_i}{\partial X_{k}} = \frac{\partial X_{k}}{\partial X_{k}} + \frac{\partial \sum_{i \ne k} X_{i}}{\partial X_{k}} = 1
\end{equation}

\begin{equation} \label{dex}
    \frac{\partial \sum_{i}e^{-\beta X_{i}}}{\partial X_{k}} = \frac{\partial e^{-\beta X_{k}}}{\partial X_{K}} + \frac{\partial \sum_{i \ne k} e^{-\beta X_{i}}}{\partial X_{k}} = -\beta e^{-\beta X_{k}}
\end{equation}

\begin{equation} \label{dxex}
\begin{aligned} 
    \frac{\partial \sum_{i}\beta X_{i} e^{-\beta X_{i}}}{\partial X_{k}} 
    &= \frac{\partial \beta X_{k} e^{-\beta X_{k}}}{\partial X_{k}} + \frac{\partial \sum_{i \ne k} \beta X_{i} e^{-\beta X_{i}}}{\partial X_{k}} \\
    &= \frac{\partial \beta X_{K}}{\partial X_{k}} e^{-\beta X_{k}} + \beta X_{k} \frac{\partial e^{-\beta X_{k}}}{\partial X_{k}}\\
    &= \beta e^{-\beta X_{k}} + \beta X_{k} (-\beta e^{-\beta X_{k}})\\
    &= \beta e^{-\beta X_{k}}(1 - \beta X_{k})
\end{aligned}
\end{equation}

\begin{equation} \label{dlne}
    \frac{\partial \ln{\sum_{j} e^{-\beta X_{j}}}}{\partial X_{k}} = \frac{1}{\sum_{j} e^{-\beta X_{j}}} \frac{\partial \sum_{j} e^{-\beta X_{j}}}{\partial X_{k}} = \frac{-\beta e^{-\beta X_{k}}}{\sum_{j}e^{-\beta X_{j}}}
\end{equation}

\begin{equation} \label{qderiv}
    \frac{\partial q_{i}}{\partial X_{i}} = \frac{-\sum_{j} e^{-\beta X_{j}} \beta e^{-\beta X_{i}} + \beta (e^{-\beta X_{i}})^2} {(\sum_{j}e^{-\beta X_{j}})^2} = \beta q_{i} \left(q_{i} - 1\right)
\end{equation}

%%%%%%%%%%%%%%%%%%%%%
% Left Derivatives
%%%%%%%%%%%%%%%%%%%%%
\subsection{Left Term Entropy Derivative}

%TODO: Lign up equal signs

\begin{displaymath}
    \frac{\partial H_{l}}{\partial X_{k,n}} =
    \frac{\sum_{j}e^{-\beta X_{j,n}}\frac{\partial\sum_{i}\beta X_{i,n} e^{-\beta X_{i,n}}}{\partial X_{k,n}}
    - \sum_{i}\beta X_{i,n} e^{-\beta X_{i,n}}\frac{\partial\sum_{j}e^{-\beta X_{j,n}}}{\partial X_{k,n}}}{\left(\sum_{j}e^{-\beta X_{j,n}}\right)^{2}}
    + \ldots
\end{displaymath}

\begin{displaymath}
    \frac{\partial H_{l}}{\partial X_{k,n}} =
    \frac{\sum_{j}e^{-\beta X_{j,n}} \beta e^{-\beta X_{k,n}}\left(1 - \beta X_{k,n}\right) + \beta^{2} e^{-\beta X_{k,n}} \sum_{i}X_{i,n}e^{-\beta X_{i,n}}}{\left(\sum_{j}e^{-\beta X_{j,n}}\right)^{2}}
    + \ldots
\end{displaymath}

\begin{displaymath}
    \frac{\partial H_{l}}{\partial X_{k,n}} =
    \frac{\beta e^{-\beta X_{k,n}}\left(1 - \beta X_{k,n}\right)}{\sum_{j}e^{-\beta X_{j,n}}}
    + \frac{\beta^{2} e^{-\beta X_{k,n}} \sum_{i}X_{i,n}e^{-\beta X_{i,n}}}{\left(\sum_{j}e^{-\beta X_{j,n}}\right)^{2}}
    + \ldots
\end{displaymath}

\begin{displaymath}
    \frac{\partial H_{l}}{\partial X_{k,n}} =
    \frac{\beta e^{-\beta X_{k,n}}}{\sum_{j}e^{-\beta X_{j,n}}} \left(1 - \beta X_{k,n} + \frac{\beta \sum_{i}X_{i,n}e^{-\beta X_{i,n}}}{\sum_{j}e^{-\beta X_{j,n}}}\right)
    + \ldots
\end{displaymath}

\begin{displaymath}
    \frac{\partial H_{l}}{\partial X_{k,n}} = \beta q_{k,n} \left(1 - \beta X_{k,n} + \beta \sum_{i}X_{i,n}q_{i,n}\right) + \ldots
\end{displaymath}

\begin{displaymath}
    \frac{\partial H_{l}}{\partial X_{k,n}} = \beta q_{k,n} - \beta^{2}X_{k,n}q_{k,n} + \beta^{2}q_{k,n}\sum_{i}X_{i,n}q_{i,n} + \ldots
\end{displaymath}

%%%%%%%%%%%%%%%%%%%%%
% Right Derivatives
%%%%%%%%%%%%%%%%%%%%%
\subsection{Right Term Entropy Derivative}

\begin{displaymath}
    \frac{\partial H_{l}}{\partial X_{k,n}} = \ldots + 
    \sum_{i}e^{-\beta x_{i,n}} \left(\frac{\sum_{j}e^{-\beta X_{j,n}} \left(\frac{-\beta e^{-\beta X_{k,n}}}{\sum_{j}e^{-\beta X_{j,n}}}\right)
    + \beta e^{-\beta X_{k,n}}\ln{\sum_{j}e^{-\beta X_{j,n}}}}{\left(\sum_{j}e^{-\beta X_{j,n}}\right)^{2}}\right)
    + \frac{\ln{\sum_{j}e^{-\beta X_{j,n}}}}{\sum_{j}e^{-\beta X_{j,n}}}\left(-\beta e^{-\beta X_{k,n}}\right)
\end{displaymath}

\begin{displaymath}
    \frac{\partial H_{l}}{\partial X_{k,n}} = \ldots + 
    \frac{-\beta e^{-\beta X_{k,n}}}{\sum_{j}e^{-\beta X_{j,n}}}
    + \frac{\beta e^{-\beta X_{k,n}}\ln{\sum_{j}e^{-\beta X_{j,n}}}}{\sum_{j}e^{-\beta X_{j,n}}}
    - \frac{\beta e^{-\beta X_{k,n}}\ln{\sum_{j}e^{-\beta X_{j,n}}}}{\sum_{j}e^{-\beta X_{j,n}}}
\end{displaymath}

\begin{equation}
    \frac{\partial H_{l}}{\partial X_{k,n}} = \ldots - \beta q_{k,n}
\label{rightd}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%
% Combine Derivatives
%%%%%%%%%%%%%%%%%%%%%
\subsection{Combine Left and Right Entropy Derivatives}

\begin{displaymath}
    \frac{\partial H_{l}}{\partial X_{k,n}} = \beta q_{k,n} - \beta^{2} X_{k,n} q_{k,n} + \beta^{2} q_{k,n} \sum_{i}X_{i,n}q_{i,n} - \beta q_{k,n}
\end{displaymath}

\begin{displaymath}
    \frac{\partial H_{l}}{\partial X_{k,n}} = - \beta^{2}X_{k,n}q_{k,n} + \beta^{2}q_{k,n}\sum_{i}X_{i,n}q_{i,n}
\end{displaymath}

\begin{equation}
    \frac{\partial H_{l}}{\partial X_{k,n}} = \beta^{2}q_{k,n}\left(\sum_{i}X_{i,n}q_{i,n} - X_{k,n}\right)
\label{combd}
\end{equation}

\noindent where $q_{k,n}$ is given in equation \ref{qdef}.

%%%%%%%%%%%%%%%%%%%%%
% Cost Derivative
%%%%%%%%%%%%%%%%%%%%%
\subsection{Loss Derivative}
\noindent Equation \ref{cost} has two entropy terms in it, the gradient of which can be derived by following the same process outlined above.

\begin{displaymath}
    \frac{\partial C}{\partial X_{k,n}} = \beta^{2} q_{k,n} \left(\sum_{i}X_{i,n}q_{i,n} - X_{k,n}\right) - \lambda \beta^{2} p_{k,n} \left( \sum_{l}X_{k,l}p_{k,l} - X_{k,n}\right)
\end{displaymath}

\begin{equation} \label{costgrad}
    \frac{\partial C}{\partial X_{k,n}} = \beta^{2} \left( q_{k,n} \left(\sum_{i}X_{i,n}q_{i,n} - X_{k,n}\right) - \lambda p_{k,n} \left( \sum_{l}X_{k,l}p_{k,l} - X_{k,n}\right)\right)
\end{equation}

% BibTeX users please use one of
%\bibliographystyle{spbasic}  % basic style, author-year citations
%\bibliographystyle{spmpsci}  % mathematics and physical sciences
%\bibliographystyle{spphys}   % APS-like style for physics
%\bibliography{references}    % name your BibTeX data base

\end{document}
